var talks = [
  [
    "Stereotyping in constrastive learning",
    "JSM 2023<br />Aug 7, 2023",
    -79.3839347,
    43.6534817
  ],
  [
    "AI fairness and robustness",
    "AAAI 2023<br />Feb 7, 2023",
    -77.0365427,
    38.8950368
  ],
  [
    "Mitigating algorithmic biases caused by distribution shifts",
    "Tech for Justice seminar<br />Jan 25, 2023",
    -71.1040018,
    42.3655767
  ],
  [
    "Mitigating algorithmic biases caused by distribution shifts",
    "ICSDS 2022<br />Dec 13, 2022",
    11.2555757,
    43.7698712
  ],
  [
    "Domain Adaptation Meets Individual Fairness. And they get along.",
    "2022 ICSA China Conference<br />Jul 2, 2022",
    108.9423443,
    34.2607713
  ],
  [
    "Model evaluation under distribution shift",
    "2022 ICSA Applied Statistics Symposium<br />Jun 21, 2022",
    -82.3249846,
    29.6519684
  ],
  [
    "Does enforcing fairness mitigate algorithmic biases due to distribution shift?",
    "EcoSta 2022<br />Jun 4, 2022",
    135.7556075,
    35.021041
  ],
  [
    "Model evaluation under distribution shift",
    "2022 ATD Annual Workshop<br />May 24, 2022",
    -77.3063733,
    38.8462236
  ],
  [
    "Statistical perspectives on federated learning",
    "NIST AI Metrology presentation series<br />Apr 21, 2022",
    -77.1929215,
    39.1399187
  ],
  [
    "Learning under label shift and posterior drift",
    "UF AI Advances & Applications seminar<br />Mar 18, 2022",
    -82.3249846,
    29.6519684
  ],
  [
    "Does enforcing fairness mitigate algorithmic biases due to distribution shift?",
    "2022 UF Statistics winter workshop<br />Jan 15, 2022",
    -82.3249846,
    29.6519684
  ],
  [
    "Does enforcing fairness mitigate algorithmic biases due to distribution shift?",
    "ICSA 2021<br />Sep 15, 2021",
    59.9823317,
    57.9110833
  ],
  [
    "There is no trade off: enforcing fairness can improve accuracy",
    "JSM 2021<br />Aug 8, 2021",
    -122.330062,
    47.6038321
  ],
  [
    "There is no trade off: enforcing fairness can improve accuracy",
    "2020 ATD Annual Workshop<br />Nov 10, 2020",
    -77.0365427,
    38.8950368
  ],
  [
    "Communicative efficient integrative regression in high dimensions",
    "SIAM MDS 2020<br />May 5, 2020",
    -84.5124602,
    39.1014537
  ],
  [
    "Training individually fair ML systems with Sensitive Subspace Robustness",
    "IISA 2019<br />Dec 28, 2019",
    72.88662753964906,
    19.08157715
  ],
  [
    "Training individually fair ML systems with Sensitive Subspace Robustness",
    "2019 Algorithms for Threat Detection (ATD) Annual Workshop<br />Oct 21, 2019",
    -77.0365427,
    38.8950368
  ],
  [
    "Training individually fair ML systems with Sensitive Subspace Robustness",
    "IBM Research - Cambridge<br />Oct 15, 2019",
    -71.1040018,
    42.3655767
  ],
  [
    "Training individually fair ML systems with Sensitive Subspace Robustness",
    "Cornell SCAN seminar<br />Sep 9, 2019",
    -76.4968019,
    42.4396039
  ],
  [
    "Statistical notions of fairness in machine learning",
    "UC Riverside statistics department seminar<br />Apr 2, 2019",
    -117.3742389,
    33.9824949
  ],
  [
    "Three vignettes in data science",
    "Fudan statistics department short course<br />Jun 2019",
    121.4691024,
    31.2323437
  ],
  [
    "Geometric inference in admixture models",
    "CMStatistics 2018<br />Dec 15, 2018",
    10.4018624,
    43.7159395
  ],
  [
    "Fast convergence of proximal Newton-type methods in high dimensions",
    "EcoSta 2018<br />Jun 21, 2018",
    114.1628131,
    22.2793278
  ],
  [
    "Interpreting the spectral embedding in spectral clustering",
    "ISNPS 2018<br />Jun 2018",
    15.310756230322482,
    40.419441649999996
  ],
  [
    "Interpreting the spectral embedding in spectral clustering",
    "Wharton statistics department seminar<br />Nov 8, 2017",
    -75.1635262,
    39.9527237
  ],
  [
    "Statistical notions of fairness in machine learning",
    "2017 IMS China<br />Jun 2017",
    108.3627211,
    22.8193063
  ],
  [
    "Fast convergence of proximal Newton-type methods in high dimensions",
    "SIAM OP17<br />May 24, 2017",
    -123.113952,
    49.2608724
  ],
  [
    "Interpreting the spectral embedding in spectral clustering",
    "10th ICSA international conference<br />Dec 22, 2016",
    121.4691024,
    31.2323437
  ],
  [
    "Feature distributed sparse regression",
    "MOPTA 2016<br />Aug 18, 2016",
    -75.3786521,
    40.6178915
  ],
  [
    "Fast convergence of proximal Newton-type methods in high dimensions",
    "WHOA PSI 2016<br />Sep 20, 2016",
    -90.1910154,
    38.6280278
  ],
  [
    "Feature distributed sparse regression",
    "SLDS 2016<br />Jun 8, 2016",
    -79.05578,
    35.9131542
  ],
  [
    "Exact post selection inference for sparse regression problems",
    "Rice statistics department seminar<br />2016",
    -95.3676974,
    29.7589382
  ],
  [
    "Communication efficient sparse regression: a one shot approach",
    "Ford Greenfield Labs<br />2016",
    -122.1598465,
    37.4443293
  ],
  [
    "Communication efficient sparse regression: a one shot approach",
    "Stanford LA/OPT seminar<br />Apr 2015",
    -122.17030545534064,
    37.426540700000004
  ],
  [
    "Distributed estimation and inference for sparse regression",
    "USC ISE department seminar<br />Feb 26, 2015",
    -118.242766,
    34.0536909
  ],
  [
    "Distributed estimation and inference for sparse regression",
    "Rutgers Department of Statistics and Biostatistics seminar<br />Feb 24, 2015",
    -74.4660408,
    40.5462553
  ],
  [
    "Distributed estimation and inference for sparse regression",
    "Chicago Booth statistics and econometrics seminar<br />Feb 2015",
    -87.6244212,
    41.8755616
  ],
  [
    "Distributed estimation and inference for sparse regression",
    "Cornell statistics department seminar<br />Feb 2015",
    -76.4968019,
    42.4396039
  ],
  [
    "Distributed estimation and inference for sparse regression",
    "Carnegie Mellon statistics seminar<br />Feb 7, 2015",
    -79.9900861,
    40.4416941
  ],
  [
    "Distributed estimation and inference for sparse regression",
    "IU Bloomington statistics department seminar<br />Feb 4, 2015",
    -86.5342881,
    39.1670396
  ],
  [
    "Distributed estimation and inference for sparse regression",
    "McGill statistics seminar<br />Jan 30, 2015",
    -73.5698065,
    45.5031824
  ],
  [
    "Distributed estimation and inference for sparse regression",
    "Michigan State CMSE colloquium<br />Jan 2015",
    -84.5553805,
    42.7337712
  ],
  [
    "Distributed estimation and inference for sparse regression",
    "USC Marshall DSO statistics seminar<br />Jan 2015",
    -118.242766,
    34.0536909
  ],
  [
    "Distributed estimation and inference for sparse regression",
    "Michigan statistics dept seminar<br />Jan 16, 2015",
    -83.7312291,
    42.2681569
  ],
  [
    "Distributed estimation and inference for sparse regression",
    "UC Santa Barbara Department of Statistics and Applied Probability seminar<br />Jan 12, 2015",
    -119.702667,
    34.4221319
  ],
  [
    "Distributed estimation and inference for sparse regression",
    "Georgia Tech ISyE seminar<br />Jan 8, 2015",
    -84.3902644,
    33.7489924
  ],
  [
    "A one shot approach to distributed sparse regression",
    "UC Davis statistics seminar<br />Jan 5, 2015",
    -121.744583,
    38.545379
  ],
  [
    "Exact inference for censored regression problems",
    "JSM 2014<br />Aug 4, 2014",
    -71.060511,
    42.3554334
  ],
  [
    "Learning mixtures of linear classifiers",
    "ICML 2014<br />Jun 23, 2014",
    116.3912972,
    39.9057136
  ],
  [
    "Proximal Newton type methods for minimizing composite functions",
    "ICCOPT 2013<br />Jul 2013",
    -9.1365919,
    38.7077507
  ]
];