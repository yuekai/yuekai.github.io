<!DOCTYPE html>
<html lang="en-US">
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Research | Yuekai Sun</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Research" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Yuekai is an assistant professor in the statistics department at the University of Michigan. His research leverages statistical science to make AI safer and more robust in the real world." />
<meta property="og:description" content="Yuekai is an assistant professor in the statistics department at the University of Michigan. His research leverages statistical science to make AI safer and more robust in the real world." />
<link rel="canonical" href="http://yuekai.github.io/research/" />
<meta property="og:url" content="http://yuekai.github.io/research/" />
<meta property="og:site_name" content="Yuekai Sun" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Research" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Yuekai is an assistant professor in the statistics department at the University of Michigan. His research leverages statistical science to make AI safer and more robust in the real world.","headline":"Research","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://yuekai.github.io/assets/img/Yuekai.jpg"}},"url":"http://yuekai.github.io/research/"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet" href="/assets/css/style.css">
  <!--[if lt IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
  <![endif]-->
  <script src="https://kit.fontawesome.com/ef661a1083.js" crossorigin="anonymous"></script>
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon/favicon.ico">
  <link rel="apple-touch-icon-precomposed" href="/assets/favicon/apple-touch-icon.png">
</head>
  <body>
    <div id="container">

      <header>
  <h1><a href="http://yuekai.github.io/">Yuekai Sun</a></h1>

  
  
  <nav>
    <ul><li>
        <a href="/research/">Research</a>
      </li><li>
        <a href="/papers/">Papers</a>
      </li><li>
        <a href="/teaching/">Teaching</a>
      </li></ul>
  </nav>
</header>

      <section>

      <h1 id="research">Research</h1>

<p>AI can match or even outperform humans in many tasks (even <a href="https://www.deepmind.com/blog/competitive-programming-with-alphacode">competitive programming</a>), but they are also prone to mistakes on unfamiliar inputs. Unfortunately, AI often encounter unfamiliar inputs when deployed in the real world because training data is seldom diverse enough to reflect the diversity of real-world inputs. This not only degrades the AI’s performance but can also unduly harm protected demographic groups. For example,</p>

<ol>
  <li>gender classification AI tends to misclassify dark-skinned females because they are underrepresented in training data [<a href="http://gendershades.org/">Buolamwini &amp; Gebru</a>];</li>
  <li>chest X-ray assessment AI performs worse when they are evaluated in hospitals that were not included in the training dataset [<a href="https://doi.org/10.1371/journal.pmed.1002683">Zech et al</a>];</li>
  <li>polygenic risk scores are many times less accurate on non-European people because most subjects in genome-wide association studies are European [<a href="https://www.nature.com/articles/s41588-019-0379-x">Martin et al</a>].</li>
</ol>

<p>My research leverages statistical science to improve the robustness of AI in the real world. Towards this goal, we work on:</p>

<h2 id="adversarial-and-distributional-robustness">Adversarial and distributional robustness</h2>

<p>AI often encounters adversarial and out-of-distribution inputs in the real world. Although there are methods for training adversarially and distributionally robust AI, many lack theoretical justification. This is a form of technical debt that hinders us from anticipating AI safety issues before they occur. We seek to repay this technical debt. Some recent papers are</p>

<ol>
  <li><strong><a href="https://openreview.net/forum?id=XSNfXG9HBAu">Domain Adaptation meets Individual Fairness. And they get along.</a></strong><br />
D Mukherjee, F Petersen, M Yurochkin, Y Sun. <em>NeurIPS 2022</em>.</li>
  <li><strong><a href="https://openreview.net/forum?id=oWqWiazEb62">Calibrated Data-Dependent Constraints with Exact Satisfaction Guarantees</a></strong><br />
S Xue, M Yurochkin, Y Sun. <em>NeurIPS 2022</em>.</li>
  <li><strong><a href="https://openreview.net/forum?id=Tv0O_cAdKtW">On sensitivity of meta-learning to support data</a></strong><br />
M Agarwal, M Yurochkin, Y Sun. <em>NeurIPS 2021</em>.</li>
</ol>

<h2 id="algorithmic-fairness">Algorithmic fairness</h2>

<p>We developed a suite of algorithms to help practitioners implement <strong>individual fairness (IF)</strong>, an intuitive notion of algorithmic fairness that requires algorithms to ``treat similar inputs similarly’’ [<a href="https://doi.org/10.1145/2090236.2090255">Dwork et al</a>]. The suite includes algorithms for:</p>

<ol>
  <li><em>aligning</em> similarity metrics with user feedback [<a href="http://www.openreview.net/pdf?id=B1gdkxHFDH">Yurochkin et al</a>, <a href="http://proceedings.mlr.press/v119/mukherjee20a.html">Mukherjee et al</a>],</li>
  <li><em>auditing</em> algorithms for (individual) unfairness [<a href="http://proceedings.mlr.press/v108/xue20a.html">Xue et al</a>, <a href="https://openreview.net/forum?id=z9k8BWL-_2u">Maity et al</a>],</li>
  <li><em>training</em> individually fair AI [<a href="https://openreview.net/forum?id=JBAa9we1AL">Vargo et al</a>, <a href="https://openreview.net/forum?id=DktZb97_Fx">Yurochkin &amp; Sun</a>, <a href="https://openreview.net/forum?id=qGeqg4_hA2">Petersen et al</a>].</li>
</ol>

<p>Before our work, IF was dismissed as impractical because there were no practical ways of picking similarity metrics for many AI tasks. The (similarity) metric learning algorithms in the suite address this issue by helping practitioners <em>align</em> similarity metrics with user feedback. To broaden IF adoption in AI practice, our colleagues at IBM Research implemented the suite of methods in the <a href="https://ibm.github.io/inFairness/">inFairness</a> package.</p>

<figure style="max-width:540px">
  <iframe width="540" height="303.75" src="https://video.ibm.com/embed/recorded/131932983" scrolling="no" allowfullscreen="" webkitallowfullscreen="" frameborder="0" style="aspect-ratio: 16/9; border: none; border-radius: 8px;"></iframe>
  <figurecaption>A tutorial on our work on individual fairness and the <a href="https://ibm.github.io/inFairness/">inFairness</a> package. </figurecaption>
</figure>

<h2 id="learning-under-distribution-shifts">Learning under distribution shifts</h2>

<p>Many instances of algorithmic bias are caused by distribution shifts (see preceding examples). To develop <em>effective</em> algorithmic fairness practices, we must address the underlying distribution shift. This led to an ongoing effort to develop <em>statistically-principled</em> methods for transfer learning and domain adaptation. Some representative papers are:</p>

<ol>
  <li><strong><a href="https://openreview.net/forum?id=DBMttEEoLbw">Understanding new tasks through the lens of training data via exponential tilting</a></strong><br />
S Maity, M Yurochkin, M Banerjee, Y Sun. <em>ICLR 2023</em>.</li>
  <li><strong><a href="https://openreview.net/forum?id=6mUrD5rg-UU">Does enforcing fairness mitigate biases caused by subpopulation shift</a></strong><br />
S Maity, D Mukherjee, M Yurochkin, Y Sun. <em>NeurIPS 2021</em>.</li>
  <li><strong><a href="https://jmlr.org/papers/v23/21-1519.html">Minimax optimal approaches to the label shift problem</a></strong><br />
S Maity, Y Sun, M Banerjee. <em>Journal of Machine Learning Research</em> (2022).</li>
</ol>


      </section>

      <footer>
  <p><i class="bullet-icon fas fa-map-marker-alt fa-fw"></i>271 West Hall<br>
  1085 S University Ave<br>
  Ann Arbor, MI 48109</p>

  <p><i class="bullet-icon fas fa-envelope fa-fw"></i>yuekai<i class="fas fa-at fa-fw"></i>umich.edu</p>

  <p><small>Hosted on GitHub | Powered by <a href="https://jekyllrb.com/">Jekyll</a></small></p>
</footer>

    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>