<!DOCTYPE html>
<html lang="en-US">
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Research | Yuekai Sun</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Research" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Yuekai is an assistant professor in the statistics department at the University of Michigan. His research leverages statistical science to make AI safer and more reliable in the real world." />
<meta property="og:description" content="Yuekai is an assistant professor in the statistics department at the University of Michigan. His research leverages statistical science to make AI safer and more reliable in the real world." />
<link rel="canonical" href="https://yuekai.github.io/research/" />
<meta property="og:url" content="https://yuekai.github.io/research/" />
<meta property="og:site_name" content="Yuekai Sun" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Research" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Yuekai is an assistant professor in the statistics department at the University of Michigan. His research leverages statistical science to make AI safer and more reliable in the real world.","headline":"Research","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://yuekai.github.io/assets/img/yuekai.jpg"}},"url":"https://yuekai.github.io/research/"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet" href="https://yuekai.github.io/assets/css/style.css">
  <!--[if lt IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
  <![endif]-->
  <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,400;0,700;1,400&display=swap" onload="this.rel='stylesheet'">
  <script defer src="https://kit.fontawesome.com/ef661a1083.js" crossorigin="anonymous"></script>
  
  <link rel="shortcut icon" type="image/x-icon" href="https://yuekai.github.io/assets/favicon/favicon.ico">
  <link rel="apple-touch-icon-precomposed" href="https://yuekai.github.io/assets/favicon/apple-touch-icon.png">
</head>
  <body>
    <div id="container">

      <header>
  <h1><a href="https://yuekai.github.io/">Yuekai Sun</a></h1>
  
  <nav>
    <ul><li>
        <a href="https://yuekai.github.io/research/">Research</a>
      </li><li>
        <a href="https://yuekai.github.io/papers/">Papers</a>
      </li><li>
        <a href="https://yuekai.github.io/teaching/">Teaching</a>
      </li></ul>
  </nav>
</header>

      <section>

      <h1 id="research">Research</h1>

<p>AI can match or even outperform humans in many tasks, but they are also prone to mistakes on unfamiliar inputs. Unfortunately, AI often encounter unfamiliar inputs when deployed in the real world because training data is seldom diverse enough to reflect the diversity of real-world inputs. This not only degrades (AI) reliability but can also lead to safety violations. For example,</p>

<ol>
  <li>polygenic risk scores are many times less accurate on non-European people because most subjects in genome-wide association studies are European [<a href="https://www.nature.com/articles/s41588-019-0379-x">Martin et al</a>];</li>
  <li>word embeddings perpetuate gender stereotypes: “man is to computer programmer as woman is to homemaker” [<a href="https://dl.acm.org/doi/10.5555/3157382.3157584">Bolukbasi et al</a>];</li>
  <li>malicious users can circumvent ChatGPT’s safety training with adversarial inputs (“jailbreak” attacks) [<a href="https://arxiv.org/abs/2307.02483">Wei et al</a>].</li>
</ol>

<p>My research leverages statistical science to improve the reliability and safety of AI in the real world. Towards this goal, we work on:</p>

<h2 id="ai-evaluation">AI evaluation</h2>

<p>Despite transformative advances in AI (experts predict that AI will exceed human capabilities in the near future [<a href="https://managing-ai-risks.com/">Bengio et al</a>]), we evaluate AI in the same ways as we evaluate simpler predictive models (eg test set accuracy). Such simple evaluation practices cannot assess rich AI outputs <em>holistically</em>, leading to blind spots that ultimately hamper AI safety and reliability. To fill this gap between AI evaluation needs and practice, we develop new statistical theory and methods for holistic AI evaluation. Some representative papers are:</p>

<ol>
  <li><a href="https://arxiv.org/abs/2412.06540"><strong>Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families</strong></a><br />
F Maia Polo, S Somerstep, L Choshen, Y Sun, M Yurochkin.</li>
  <li><a href="https://openreview.net/forum?id=8wzYdXWYJv"><strong>Efficient multi-prompt evaluation of LLMs</strong></a><br />
F Maia Polo, R Xu, L Weber, M Silva, O Bhardwaj, L Choshen, A Oliveira, Y Sun, M Yurochkin. <em>NeurIPS 2024</em>.</li>
  <li><a href="https://openreview.net/forum?id=qAml3FpfhG"><strong>tinyBenchmarks: evaluating LLMs with few examples</strong></a><br />
F Maia Polo, L Weber, L Choshen, Y Sun, G Xu, M Yurochkin. <em>ICML 2024</em>.</li>
</ol>

<h2 id="algorithmic-fairness">Algorithmic fairness</h2>

<p>We take a statistical approach to algorithmic fairness: algorithmic biases are often caused by distribution shifts (eg due to sampling biases in the training data), which suggests they are statistical problems that admit statistical solutions. A key insight from our work is (contrary to popular belief) aligning AI so that they are fair/safe/transparent may not be at odds with accuracy. In fact, <em>alignment can improve (out-of-distribution) performance</em>. Sometimes, we can even exploit the distribution shifts to achieve otherwise unachievable fairness objectives (eg achieve equality of outcomes with (policies that satisfy) equality of opportunity/treatment). This motivates some of our work on distribution shifts. Here are some representative papers:</p>

<ol>
  <li><a href="https://dl.acm.org/doi/10.1145/3630106.3658929"><strong>Algorithmic Fairness in Performative Policy Learning: Escaping the  Impossibility of Group Fairness</strong></a><br />
S Somerstep, Y Ritov, Y Sun. <em>FAccT 2024</em>.</li>
  <li><a href="https://openreview.net/forum?id=q4SiDyYQbo"><strong>An Investigation of Representation and Allocation Harms in Contrastive Learning</strong></a><br />
S Maity, M Agarwal, M Yurochkin, Y Sun. <em>ICLR 2024</em>.</li>
  <li><a href="https://openreview.net/forum?id=XSNfXG9HBAu"><strong>Domain Adaptation meets Individual Fairness. And they get along.</strong></a><br />
D Mukherjee, F Petersen, M Yurochkin, Y Sun. <em>NeurIPS 2022</em>.</li>
</ol>

<p>We also developed a suite of algorithms to help practitioners implement <strong>individual fairness (IF)</strong>, an intuitive notion of algorithmic fairness that requires algorithms to “treat similar inputs similarly”. The suite includes algorithms for aligning similarity metrics with user feedback, auditing algorithms for violations of IF, and training individually fair AI. Before our work, IF was dismissed as impractical because there were no similarity metrics for many AI tasks and no practical algorithms to train individually fair AI. We addressed both issues with the (similarity) metric learning and training algorithms in the suite. Try the algorithms in IBM’s <a href="https://github.com/Trusted-AI/AIF360">AIF360 toolkit</a>!</p>

<h2 id="transfer-learning">Transfer learning</h2>

<p>AI often encounter adversarial and out-of-distribution inputs in the real world, but the pernicious effects of distribution shifts are poorly understood. This is a form of technical debt that hinders us from anticipating AI risks before they arise. We seek to repay this technical debt. Some representative papers are:</p>

<ol>
  <li><a href="https://arxiv.org/abs/2405.15172"><strong>Learning the Distribution Map in Reverse Causal Performative Prediction</strong></a><br />
D Bracale, S Maity, S Somerstep, M Banerjee, Y Sun. to appear in <em>AISTATS 2025</em>.</li>
  <li><a href="https://openreview.net/forum?id=PeLLMw3wLX"><strong>A transfer learning framework for weak-to-strong generalization</strong></a><br />
S Somerstep, F Maia Polo, M Banerjee, Y Ritov, M Yurochkin, Y Sun. <em>ICLR 2025</em>.</li>
  <li><a href="https://openreview.net/forum?id=DBMttEEoLbw"><strong>Understanding new tasks through the lens of training data via exponential tilting</strong></a><br />
S Maity, M Yurochkin, M Banerjee, Y Sun. <em>ICLR 2023</em>.</li>
</ol>

<figure style="max-width:540px">
  <iframe width="540" src="https://www.youtube.com/embed/q68VtFoWmFI?si=_Ct_VCYD6GcrMzb8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" style="aspect-ratio: 16/9;"></iframe>
</figure>


      </section>

      <footer>
  <p>
    <i class="bullet-icon fas fa-map-marker-alt fa-fw"></i>271 West Hall<br>
    1085 S University Ave<br>
    Ann Arbor, MI 48109
  </p>
  <p><i class="bullet-icon fas fa-envelope fa-fw"></i>yuekai<i class="fas fa-at fa-fw"></i>umich.edu</p>
  <p><small>Hosted on <a href="http://github.com">GitHub</a> | Made with <a href="http://jekyllrb.com/">Jekyll</a></small></p>
</footer>

    </div>
    <script src="https://yuekai.github.io/assets/js/scale.fix.js"></script>
    
    
  </body>
</html>