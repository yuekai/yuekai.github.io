<!DOCTYPE html>
<html lang="en-US">
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Convergence of SVRG | Yuekai Sun</title>
<meta name="generator" content="Jekyll v4.0.1" />
<meta property="og:title" content="Convergence of SVRG" />
<meta name="author" content="Yuekai Sun" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post accompanies the slides on variance reduction methods. Please see the slides for the problem setup and definitions." />
<meta property="og:description" content="This post accompanies the slides on variance reduction methods. Please see the slides for the problem setup and definitions." />
<link rel="canonical" href="http://yuekai.github.io/posts/2020-04-16-svrg-convergence" />
<meta property="og:url" content="http://yuekai.github.io/posts/2020-04-16-svrg-convergence" />
<meta property="og:site_name" content="Yuekai Sun" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-16T00:00:00-07:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Yuekai Sun"},"description":"This post accompanies the slides on variance reduction methods. Please see the slides for the problem setup and definitions.","headline":"Convergence of SVRG","dateModified":"2020-04-16T00:00:00-07:00","datePublished":"2020-04-16T00:00:00-07:00","url":"http://yuekai.github.io/posts/2020-04-16-svrg-convergence","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://yuekai.github.io/posts/2020-04-16-svrg-convergence"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet" href="/assets/css/style.css">
  <!--[if lt IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
  <![endif]-->
  <script src="https://kit.fontawesome.com/a076d05399.js"></script>
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon/favicon.ico">
  <link rel="apple-touch-icon-precomposed" href="/assets/favicon/apple-touch-icon.png">
  
  <script src="/assets/js/load-mathjax.js" async></script>
  
</head>
  <body>
    <div class="wrapper">

      <header>
  <h1><a href="http://yuekai.github.io/">Yuekai Sun</a></h1>

  
  
  <ul id="nav-bar"><li>
        <a href="/courses/">Courses</a>
      </li><li>
        <a href="/papers/">Papers</a>
      </li><li>
        <a href="/talks/">Talks</a>
      </li></ul>
</header>

      <section>

      <h2>Convergence of SVRG</h2>

<blockquote>
  <p>This post accompanies the <a href="http://www-personal.umich.edu/~yuekai/stats-606/SVRG.pdf">slides on variance reduction methods</a>. Please see the slides for the problem setup and definitions.</p>
</blockquote>

<p>To keep the proof simple, we study the convergence of a version of SVRG that picks an anchor point uniformly at random from the iterates of the previous epoch:</p>

\[x_s \sim \unif\{x_{s-1,0},\dots,s_{s-1,m-1}\}.\]

<p><strong>Theorem (linear convergence of SVRG):</strong> Let the \(f_i\)â€™s be convex and \(L\)-strongly smooth and \(F\) be \(\mu\)-strongly convex. For any step size \(\eta &gt; 0\), as long as the epoch length \(m\) is large enough so that the contraction factor \(\gamma \triangleq \frac{1}{\mu\eta(1-2L\eta)m} + \frac{2L\eta}{1-2L\eta}\) is less than 1, the major iterates of SVRG converge linearly:</p>

\[\Ex\big[F(x_s) - F_*\big] \le \gamma^s(F(x_{0,0}) - F_*).\]

<p><strong>Proof:</strong> Define \(\Ex_{s,t}\) as expectation conditioned on the outputs of the stochastic gradient oracle up till the end of the \(t-1\)-th iteration of the \(s\)-th epoch and</p>

\[g_{s,t} \triangleq \partial f_{i_{s,t}}(x_{s,t}) - \partial f_{i_{s,t}}(x_s) + \partial F(x_s)\]

<p>as the SVRG gradient estimate. We expand the SVRG update rule to obtain</p>

\[\begin{aligned}
\Ex_{s,t}\big[\|x_{s,t+1} - x_*\|_2^2\big] &amp;= \Ex\big[\|x_{s,t} - \eta g_{s,t} - x_*\|_2^2\big] \\
&amp;= \|x_{s,t} - x_*\|_2^2 - 2\eta\Ex_{s,t}\langle g_{t,s},x_{s,t} - x_*\rangle + \eta^2\Ex_{s,t}\big[\|g_{s,t}\|_2^2\big] \\
&amp;= \|x_{s,t} - x_*\|_2^2 - 2\eta\langle\partial F(x_{t,s}),x_{s,t} - x_*\rangle + \eta^2\Ex_{s,t}\big[\|g_{s,t}\|_2^2\big] \\
&amp;\le \|x_{s,t} - x_*\|_2^2 - 2\eta(F(x_{s,t}) - F_*) + \eta^2\Ex_{s,t}\big[\|g_{s,t}\|_2^2\big],
\end{aligned}\]

<p>where we appealed to the unbiasedness of \(g_{s,t}\) in the third step and the convexity of \(F\) in the fourth step. We expand \(\Ex_{s,t}\big[\|g_{s,t}\|_2^2\big]\) to obtain</p>

\[\begin{aligned}
\Ex_{s,t}\big[\|g_{s,t}\|_2^2\big] &amp;= \Ex_{s,t}\big[\|\partial f_{i_{s,t}}(x_{s,t}) - \partial f_{i_{s,t}}(x_s) + \partial F(x_s)\|_2^2\big] \\
&amp;= \Ex_{s,t}\big[\|\partial f_{i_{s,t}}(x_{s,t}) - \partial f_{i_{s,t}}(x_*) - (\partial f_{i_{s,t}}(x_s) - \partial f_{i_{s,t}}(x_*) - \partial F(x_s))\|_2^2\big] \\
&amp;\le 2\Ex_{s,t}\big[\|\partial f_{i_{s,t}}(x_{s,t}) - \partial f_{i_{s,t}}(x_*) \|_2\big] \\
&amp;\quad+ 2\Ex_{s,t}\big[\|\partial f_{i_{s,t}}(x_s) - \partial f_{i_{s,t}}(x_*) - \partial F(x_s)\|_2^2\big] \\
&amp;= 2\Ex_{s,t}\big[\|\partial f_{i_{s,t}}(x_{s,t}) - \partial f_{i_{s,t}}(x_*) \|_2\big] \\
&amp;\quad+ 2\Ex_{s,t}\big[\|\partial f_{i_{s,t}}(x_s) - \partial f_{i_{s,t}}(x_*) - \Ex_{s,t}\big[\partial f_{i_{s,t}}(x_s) - \partial f_{i_{s,t}}(x_*)\big]\|_2^2\big] \\
&amp;\le 2\Ex_{s,t}\big[\|\partial f_{i_{s,t}}(x_{s,t}) - \partial f_{i_{s,t}}(x_*) \|_2\big] + 2\Ex_{s,t}\big[\|\partial f_{i_{s,t}}(x_s) - \partial f_{i_{s,t}}(x_*) \|_2\big],
\end{aligned}\]

<p>where we plugged</p>

\[\partial F(x_s) = \partial F(x_s) - \partial F(x_*) = \Ex_{s,t}\big[\partial f_{i_{s,t}}(x_s) - f_{i_{s,t}}(x_*)\big]\]

<p>into the fourth step. To bound the right side of the preceding expression, we appeal to the convexity and strong smoothness of \(f_i\). The \(L\)-strong smoothness of \(f_i\) implies</p>

\[\textstyle\frac{1}{2L}\|\partial f_i(x) - \partial f_i(x_*)\|_2^2 \le f_i(x) - f_i(x_*) - \langle\partial f_i(x_*),x-x_*\rangle.\]

<p>We sum over \(i\in[n]\) to obtain</p>

\[\begin{aligned}
\textstyle\frac{1}{2L}\sum_{i=1}^n\|\partial f_i(x) - \partial f_i(x_*)\|_2^2 &amp;\le nF(x) - nF(x_*) - n\langle\partial F(x_*),x-x_*\rangle \\
&amp;= n(F(x) - F_*).
\end{aligned}\]

<p>Thus the right side of bound on \(\Ex_{s,t}\big[\|g_{s,t}\|_2^2\big]\) is at most</p>

\[\Ex_{s,t}\big[\|g_{s,t}\|_2^2\big] \le 4L(F(x_{s,t}) - F_* + F(x_s) - F_*).\]

<p><em>This is the crucial result in this analysis of SVRG: it shows that size of the SVRG gradient estimate decreases as the iterates approach the optimal point.</em> We plug this bound into the expansion of the SVRG update rule to obtain</p>

\[\Ex_{s,t}\big[\|x_{s,t+1} - x_*\|_2^2\big] \le \|x_{s,t} - x_*\|_2^2 - 2\eta(1-2\eta)(F(x_{s,t}) - F_*) + 4L\eta^2(F(x_s) - F_*).\]

<p>Rearranging,</p>

\[\Ex_{s,t}\big[\|x_{s,t+1} - x_*\|_2^2\big] + 2\eta(1-2\eta)(F(x_{s,t}) - F_*) \le \|x_{s,t} - x_*\|_2^2 + 4L\eta^2(F(x_s) - F_*).\]

<p>Let \(\Ex_s \triangleq \Ex_{s,0}\) be the expectation conditioned on the outputs of the stochastic gradient oracle up till the end of the \(s-1\) epoch.
We sum and take the expectation conditioned on the outputs of the stochastic gradient oracle in the \(s\)-th epoch to obtain a bound on the progress of SVRG in an epoch:</p>

\[\begin{aligned}
&amp;\textstyle\Ex_s\big[\|x_{s,m} - x_*\|_2^2\big] + 2\eta(1-2\eta)\sum_{t=0}^m\Ex_s\big[F(x_{s,t}) - F_*\big] \\
&amp;\quad\le \Ex_s\big[\|x_s - x_*\|_2^2\big] + 4Lm\eta^2(F(x_s) - F_*) \\
&amp;\textstyle\quad\le (\frac2\mu + 4Lm\eta^2)\Ex\big[F(x_s) - F(x_*)\big],
\end{aligned}\]

<p>where we appealed to the strong convexity of \(F\) in the second step. The left side of the preceding expression is at least</p>

\[\begin{aligned}
&amp;\textstyle\Ex_s\big[\|x_{s,m} - x_*\|_2^2\big] + 2\eta(1-2\eta)\sum_{t=0}^m\Ex_s\big[F(x_{s,t}) - F_*\big] \\
&amp;\textstyle\quad\ge 2\eta(1-2\eta)\sum_{t=0}^m\Ex_s\big[F(x_{s,t}) - F_*\big] \\
&amp;\quad\ge 2\eta(1-2\eta)m\Ex_s\big[F(x_{s+1}) - F_*\big],
\end{aligned}\]

<p>where we recalled \(x_{s+1} \sim \unif\{x_{s,0},\dots,s_{s,m-1}\}\) in the second step. Thus</p>

\[\textstyle
2\eta(1-2\eta)m\Ex_s\big[F(x_{s+1}) - F_*\big] \le (\frac2\mu + 4Lm\eta^2)\Ex\big[F(x_s) - F(x_*)\big].\]

<p>Rearranging,</p>

\[\textstyle\Ex_s\big[F(x_{s+1}) - F_*\big] \le \frac{\frac2\mu + 4Lm\eta^2}{2\eta(1-2\eta)m}\Ex_s\big[F(x_s) - F(x_*)\big].\]

<p>We recognize the constant in front of \(\Ex_s\big[F(x_s) - F(x_*)\big]\) on the right side as the contraction factor \(\gamma\) to complete the proof.</p>


<p class="de-emph">Posted on April 16, 2020

from San Francisco, CA

</p>


      </section>

      <footer>
  <p><i class="bullet-icon fas fa-map-marker-alt fa-fw"></i>271 West Hall<br>
  1085 S University Ave<br>
  Ann Arbor, MI 48109</p>

  <p><i class="bullet-icon fas fa-phone-alt fa-fw"></i>734-764-4032</p>

  <p><i class="bullet-icon fas fa-envelope fa-fw"></i>yuekai<i class="fas fa-at fa-fw"></i>umich.edu</p>

  <p><small>Hosted on GitHub | Powered by <a href="https://jekyllrb.com/">Jekyll</a></small></p>
</footer>

    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>
